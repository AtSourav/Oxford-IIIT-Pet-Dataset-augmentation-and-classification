{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d2a0274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22e6f31",
   "metadata": {},
   "source": [
    "# Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71c1a755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['saint_bernard_188.jpg', 'Ragdoll_164.jpg', 'chihuahua_75.jpg', 'american_bulldog_60.jpg', 'Siamese_9.jpg', 'saint_bernard_122.jpg', 'Bombay_95.jpg', 'Bengal_167.jpg', 'Birman_95.jpg', 'havanese_147.jpg']\n",
      "\n",
      "7390\n"
     ]
    }
   ],
   "source": [
    "list_images = []\n",
    "\n",
    "with open('annotations_aug/images_list_orig.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        name = line[:-1]\n",
    "        list_images.append(name)\n",
    "        \n",
    "print(list_images[:10])\n",
    "print()\n",
    "print(len(list_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63fc0348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File name</th>\n",
       "      <th>Class id</th>\n",
       "      <th>Species id</th>\n",
       "      <th>Breed id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abyssinian_100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abyssinian_101</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abyssinian_102</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abyssinian_103</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abyssinian_104</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7344</th>\n",
       "      <td>yorkshire_terrier_96</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7345</th>\n",
       "      <td>yorkshire_terrier_97</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7346</th>\n",
       "      <td>yorkshire_terrier_98</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>yorkshire_terrier_99</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>yorkshire_terrier_9</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7349 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 File name  Class id  Species id  Breed id\n",
       "0           Abyssinian_100         1           1         1\n",
       "1           Abyssinian_101         1           1         1\n",
       "2           Abyssinian_102         1           1         1\n",
       "3           Abyssinian_103         1           1         1\n",
       "4           Abyssinian_104         1           1         1\n",
       "...                    ...       ...         ...       ...\n",
       "7344  yorkshire_terrier_96        37           2        25\n",
       "7345  yorkshire_terrier_97        37           2        25\n",
       "7346  yorkshire_terrier_98        37           2        25\n",
       "7347  yorkshire_terrier_99        37           2        25\n",
       "7348   yorkshire_terrier_9        37           2        25\n",
       "\n",
       "[7349 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_to_list = 'annotations/list.txt'\n",
    "\n",
    "annotations_df = pd.read_csv(path_to_list, sep=\" \", header=None, skiprows=6, names=[\"File name\", \"Class id\", \"Species id\", \"Breed id\"])\n",
    "\n",
    "display(annotations_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1eb39d",
   "metadata": {},
   "source": [
    "The list of annotations above is missing some images as there are 7390 images in total in the images folder. So we'll read the filenames from the images directory and make a more complete annotations_df. But first we want a dictionary relating the species and breed name to the (class id,species id,breed id)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc5ac73",
   "metadata": {},
   "source": [
    "First we define a function that will take the file name and output the corresponding Species:breed. The file names are of the following format: breed_number.ext with the first letter being in upper case for cats and lower case for dogs. For example, the file name of an image of an Abyssinian cat is like 'Abyssinian_34.jpg' (or .png or .gif) and we want the output to be 'Cat: abyssinian'. Similarly the file name for an image of a Beagle dog is like 'beagle_26.jpg' and we want the output to be 'Dog: beagle'.  \n",
    "\n",
    "Later when we augment the training set, we'll name the edited images as 'name_am.jpg' where 'name' is the original name. So we'll design the format_name function to work on such file names as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee3ebaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat: abyssinian\n",
      "\n",
      "Cat: abyssinian\n"
     ]
    }
   ],
   "source": [
    "def cat_or_dog(inp):       # inp is supposed to be the file (breed) name minus the numbers, _, and the extension.\n",
    "    if inp[0].isupper():\n",
    "        return 'Cat: '+ inp.lower()\n",
    "    else:\n",
    "        return 'Dog: '+ inp\n",
    "\n",
    "def format_name(inp):\n",
    "    inp = os.path.splitext(inp)[0]\n",
    "    out = re.sub('_',' ',inp)\n",
    "    out = re.sub(r'[0-9]+','',out)\n",
    "    \n",
    "    if inp.endswith('am'):\n",
    "        out = out[:-4]\n",
    "    else:\n",
    "        out = out[:-1]\n",
    "    \n",
    "    return cat_or_dog(out)\n",
    "\n",
    "print(format_name('Abyssinian_100.png'))\n",
    "print()\n",
    "print(format_name('Abyssinian_93_am.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ac3de7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cat: abyssinian': (1, 1, 1),\n",
       " 'Dog: american bulldog': (2, 2, 1),\n",
       " 'Dog: american pit bull terrier': (3, 2, 2),\n",
       " 'Dog: basset hound': (4, 2, 3),\n",
       " 'Dog: beagle': (5, 2, 4),\n",
       " 'Cat: bengal': (6, 1, 2),\n",
       " 'Cat: birman': (7, 1, 3),\n",
       " 'Cat: bombay': (8, 1, 4),\n",
       " 'Dog: boxer': (9, 2, 5),\n",
       " 'Cat: british shorthair': (10, 1, 5),\n",
       " 'Dog: chihuahua': (11, 2, 6),\n",
       " 'Cat: egyptian mau': (12, 1, 6),\n",
       " 'Dog: english cocker spaniel': (13, 2, 7),\n",
       " 'Dog: english setter': (14, 2, 8),\n",
       " 'Dog: german shorthaired': (15, 2, 9),\n",
       " 'Dog: great pyrenees': (16, 2, 10),\n",
       " 'Dog: havanese': (17, 2, 11),\n",
       " 'Dog: japanese chin': (18, 2, 12),\n",
       " 'Dog: keeshond': (19, 2, 13),\n",
       " 'Dog: leonberger': (20, 2, 14),\n",
       " 'Cat: maine coon': (21, 1, 7),\n",
       " 'Dog: miniature pinscher': (22, 2, 15),\n",
       " 'Dog: newfoundland': (23, 2, 16),\n",
       " 'Cat: persian': (24, 1, 8),\n",
       " 'Dog: pomeranian': (25, 2, 17),\n",
       " 'Dog: pug': (26, 2, 18),\n",
       " 'Cat: ragdoll': (27, 1, 9),\n",
       " 'Cat: russian blue': (28, 1, 10),\n",
       " 'Dog: saint bernard': (29, 2, 19),\n",
       " 'Dog: samoyed': (30, 2, 20),\n",
       " 'Dog: scottish terrier': (31, 2, 21),\n",
       " 'Dog: shiba inu': (32, 2, 22),\n",
       " 'Cat: siamese': (33, 1, 11),\n",
       " 'Cat: sphynx': (34, 1, 12),\n",
       " 'Dog: staffordshire bull terrier': (35, 2, 23),\n",
       " 'Dog: wheaten terrier': (36, 2, 24),\n",
       " 'Dog: yorkshire terrier': (37, 2, 25)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_dict = {}\n",
    "for i in range(len(annotations_df)):\n",
    "    if format_name(annotations_df.iloc[i,0]) not in annotations_dict:\n",
    "        annotations_dict[format_name(annotations_df.iloc[i,0])] = tuple(annotations_df.iloc[i,1:])\n",
    "annotations_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab91987",
   "metadata": {},
   "source": [
    "# Train:valid:test = 70:15:15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75608c4",
   "metadata": {},
   "source": [
    "We have 7390 images (before augmentation). Now we'll make a random split of the images into three sets: train, validation, test, in the ratio 3:1:1 approximately.  We don't want any of the newly created images in the validation or test set. Afterwards we'll add some of the augmented images the train data set to create several data sets with different proportions of original and edited images, aiming to achieve the train:valid:test ratio mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f1d682",
   "metadata": {},
   "source": [
    "Here below and later as well, we'll often fix the random seed so we get the same results when the code is run again. To really ensure pseudorandomness however, we shouldn't do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ecfe1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set has 4449 images which is 60.20 % of the dataset.\n",
      "The validation set has 1491 images which is 20.18 % of the dataset.\n",
      "The test set has 1450 images which is 19.62 % of the dataset.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "images_train = []\n",
    "images_valid = []\n",
    "images_test = []\n",
    "for img in list_images:\n",
    "    choice = np.random.choice([1,2,3],p=[0.6,0.2,0.2])  \n",
    "    if choice == 1:\n",
    "        images_train.append(img)\n",
    "    elif choice == 2:\n",
    "        images_valid.append(img)\n",
    "    elif choice == 3:\n",
    "        images_test.append(img)\n",
    "        \n",
    "\n",
    "print('The training set has {} images which is {:.2f} % of the dataset.'.format(len(images_train),100*(len(images_train)/len(list_images))))\n",
    "print('The validation set has {} images which is {:.2f} % of the dataset.'.format(len(images_valid),100*(len(images_valid)/len(list_images))))\n",
    "print('The test set has {} images which is {:.2f} % of the dataset.'.format(len(images_test),100*(len(images_test)/len(list_images))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac7a8d9",
   "metadata": {},
   "source": [
    "Now we'll prepare a dataframe with the annotations for each of these three sets of images. We'll do it by defining a class with some attributes carrying the different annotations, and a method to return the df organising all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bda7ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "class return_annotations():\n",
    "    def __init__(self,collection):\n",
    "        self.collection = collection\n",
    "        self.names = list(map(lambda x:format_name(x),self.collection))\n",
    "        self.class_ids = [annotations_dict[pet][0] for pet in self.names]\n",
    "        self.species_ids = [annotations_dict[pet][1] for pet in self.names]\n",
    "        self.breed_ids = [annotations_dict[pet][2] for pet in self.names]\n",
    "        \n",
    "    def create_df(self):\n",
    "        return pd.DataFrame({'Image file':self.collection,'Pet':self.names,'Class id':self.class_ids,'Species id':self.species_ids,'Breed id':self.breed_ids})\n",
    "\n",
    "        \n",
    "annot_train = return_annotations(images_train)\n",
    "annot_valid = return_annotations(images_valid)\n",
    "annot_test = return_annotations(images_test)\n",
    "\n",
    "annot_train_df = annot_train.create_df()\n",
    "annot_valid_df = annot_valid.create_df()\n",
    "annot_test_df = annot_test.create_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfa91ee",
   "metadata": {},
   "source": [
    "Let us export the validation and test dataframes as csv files. We'll still be modifying the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87279dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_valid_df.to_csv('annotations_aug/annotations_valid.csv',index=False)\n",
    "annot_test_df.to_csv('annotations_aug/annotations_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03923e0b",
   "metadata": {},
   "source": [
    "# Three augmented training sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc26780e",
   "metadata": {},
   "source": [
    "We started out with roughly $200$ images per class, we moved about $40$ of each into the validation set, about another $40$ into the test set, and we augmented the remaining $120$ to create roughly $240$ images per class. We'll try to have about $190$ images per class in the training dataset, so we have approximately a $70:15:15$ train:valid:test ration. We'll make three different datasets wherein we'll make up the $190$ images per class from the original and the edited images in different proportions. \n",
    "\n",
    "We can think of this ratio of original to edited images as another hyperparameter. We'll go for $r_1= 60:40$, $r_2=50:50$, and $r_3=40:60$. For the set $i$, the number of original images included should be about $\\frac{190 r_i}{1+r_i}$ and the number of edited ones about $\\frac{190}{1+ r_i}$. Therefore each of the original images in the training should have a $\\frac{19 r_i}{12(1+r_i)}$ chance of getting into the training set, and each edited image should have a $\\frac{19}{12(1+r_i)}$ chance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c494054",
   "metadata": {},
   "outputs": [],
   "source": [
    "avoid_trimaps = []\n",
    "\n",
    "with open('annotations_aug/avoid_trimaps.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        name = line[:-1]\n",
    "        avoid_trimaps.append(name)\n",
    "\n",
    "\n",
    "def create_train_set(r):\n",
    "    t_set = []\n",
    "    for img in annot_train_df['Image file']:\n",
    "        if np.random.choice([True,False],p=[(19*r)/(12*(1+r)),1-((19*r)/(12*(1+r)))]):\n",
    "            t_set.append(img)\n",
    "        if os.path.splitext(img)[0] not in avoid_trimaps:\n",
    "            if np.random.choice([True,False],p=[19/(12*(1+r)),1-(19/(12*(1+r)))]):\n",
    "                t_set.append(os.path.splitext(img)[0]+'_am.jpg')\n",
    "            \n",
    "    t_annot = return_annotations(t_set)\n",
    "    return t_annot.create_df()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "702c4090",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "train_r1_df = create_train_set(1.5)\n",
    "train_r1_df.to_csv('annotations_aug/annotations_train_r1.csv',index=False)\n",
    "\n",
    "np.random.seed(20000)\n",
    "train_r2_df = create_train_set(1)\n",
    "train_r2_df.to_csv('annotations_aug/annotations_train_r2.csv',index=False)\n",
    "\n",
    "np.random.seed(50000)\n",
    "train_r3_df = create_train_set(2/3)\n",
    "train_r3_df.to_csv('annotations_aug/annotations_train_r3.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a7efdd",
   "metadata": {},
   "source": [
    "Now let's get summarize the datasets available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6044f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(train_df): \n",
    "    \n",
    "    len_train = len(train_df)\n",
    "    len_valid = len(images_valid)\n",
    "    len_test = len(images_test)\n",
    "    total = len_train + len_valid + len_test\n",
    "    \n",
    "    number_edited = sum([os.path.splitext(file)[0].endswith('am') for file in train_df['Image file']])\n",
    "    number_original = len_train - number_edited\n",
    "    \n",
    "    print('There are {}, {}, and {} images in the training, validation, and test sets, respectively.'.format(len_train, len_valid, len_test))\n",
    "    print()\n",
    "    print('The train:valid:test ratio is roughly {:.2f}:{:.2f}:{:.2f}.'.format(len_train*100/total, len_valid*100/total, len_test*100/total))\n",
    "    print()\n",
    "    print('The ratio of original images to the edited ones is about {:.2f}:{:.2f}.'.format(number_original*100/len_train,number_edited*100/len_train))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "963bbfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7109, 1491, and 1450 images in the training, validation, and test sets, respectively.\n",
      "\n",
      "The train:valid:test ratio is roughly 70.74:14.84:14.43.\n",
      "\n",
      "The ratio of original images to the edited ones is about 59.46:40.54.\n"
     ]
    }
   ],
   "source": [
    "summarize(train_r1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02f3461c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6983, 1491, and 1450 images in the training, validation, and test sets, respectively.\n",
      "\n",
      "The train:valid:test ratio is roughly 70.36:15.02:14.61.\n",
      "\n",
      "The ratio of original images to the edited ones is about 50.49:49.51.\n"
     ]
    }
   ],
   "source": [
    "summarize(train_r2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17daaf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7102, 1491, and 1450 images in the training, validation, and test sets, respectively.\n",
      "\n",
      "The train:valid:test ratio is roughly 70.72:14.85:14.44.\n",
      "\n",
      "The ratio of original images to the edited ones is about 40.78:59.22.\n"
     ]
    }
   ],
   "source": [
    "summarize(train_r3_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88c1472",
   "metadata": {},
   "source": [
    "Let's also create another training dataset without rejecting any of the images available in the training set in annot_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdb842bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8885, 1491, and 1450 images in the training, validation, and test sets, respectively.\n",
      "\n",
      "The train:valid:test ratio is roughly 75.13:12.61:12.26.\n",
      "\n",
      "The ratio of original images to the edited ones is about 50.07:49.93.\n"
     ]
    }
   ],
   "source": [
    "t_larger_set = []\n",
    "for img in annot_train_df['Image file']:\n",
    "    t_larger_set.append(img)\n",
    "    if os.path.splitext(img)[0] not in avoid_trimaps:\n",
    "        t_larger_set.append(os.path.splitext(img)[0]+'_am.jpg')\n",
    "            \n",
    "t_larger_annot = return_annotations(t_larger_set)\n",
    "t_larger_annot_df = t_larger_annot.create_df()\n",
    "\n",
    "t_larger_annot_df.to_csv('annotations_aug/annotations_train_240approx.csv',index=False)\n",
    "\n",
    "summarize(t_larger_annot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd7ce54",
   "metadata": {},
   "source": [
    "# Larger training sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8970ca",
   "metadata": {},
   "source": [
    "The training sets above seem to be inadequate for the models to learn to distinguish between different breeds (this is reasonable as different breeds in the same species are largely similar). So we will create some datasets with larger training sets and smaller validation and test sets and see if the performance is improved. \n",
    "\n",
    "It would be better to use K-fold cross validation, but we'll not go into that in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0917443f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set has 5940 images which is 80.38 % of the dataset.\n",
      "The validation set has 919 images which is 12.44 % of the dataset.\n",
      "The test set has 531 images which is 7.19 % of the dataset.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "imgL_train = []\n",
    "imgL_valid = []\n",
    "imgL_test = []\n",
    "for img in list_images:\n",
    "    choice = np.random.choice([1,2,3],p=[0.8,0.125,0.075])  \n",
    "    if choice == 1:\n",
    "        imgL_train.append(img)\n",
    "    elif choice == 2:\n",
    "        imgL_valid.append(img)\n",
    "    elif choice == 3:\n",
    "        imgL_test.append(img)\n",
    "        \n",
    "\n",
    "print('The training set has {} images which is {:.2f} % of the dataset.'.format(len(imgL_train),100*(len(imgL_train)/len(list_images))))\n",
    "print('The validation set has {} images which is {:.2f} % of the dataset.'.format(len(imgL_valid),100*(len(imgL_valid)/len(list_images))))\n",
    "print('The test set has {} images which is {:.2f} % of the dataset.'.format(len(imgL_test),100*(len(imgL_test)/len(list_images))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf03f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotL_train = return_annotations(imgL_train)\n",
    "annotL_valid = return_annotations(imgL_valid)\n",
    "annotL_test = return_annotations(imgL_test)\n",
    "\n",
    "annotL_train_df = annotL_train.create_df()\n",
    "annotL_valid_df = annotL_valid.create_df()\n",
    "annotL_test_df = annotL_test.create_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f52028c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotL_valid_df.to_csv('annotations_aug/annotations_valid_L.csv',index=False)\n",
    "annotL_test_df.to_csv('annotations_aug/annotations_test_L.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63b53430",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_set_L1 = []\n",
    "for img in annotL_train_df['Image file']:\n",
    "    t_set_L1.append(img)\n",
    "    if os.path.splitext(img)[0] not in avoid_trimaps:\n",
    "        if np.random.choice([True,False],p=[0.875,0.125]):\n",
    "            t_set_L1.append(os.path.splitext(img)[0]+'_am.jpg')\n",
    "            \n",
    "train_L1 = return_annotations(t_set_L1)\n",
    "train_L1_df = train_L1.create_df()\n",
    "train_L1_df.to_csv('annotations_aug/annotations_train_L1.csv',index=False)\n",
    "\n",
    "t_set_L2 = []\n",
    "for img in annotL_train_df['Image file']:\n",
    "    if np.random.choice([True,False],p=[0.875,0.125]):\n",
    "        t_set_L2.append(img)\n",
    "    if os.path.splitext(img)[0] not in avoid_trimaps:\n",
    "        t_set_L2.append(os.path.splitext(img)[0]+'_am.jpg')\n",
    "            \n",
    "train_L2 = return_annotations(t_set_L2)\n",
    "train_L2_df = train_L2.create_df() \n",
    "train_L2_df.to_csv('annotations_aug/annotations_train_L2.csv',index=False)\n",
    "\n",
    "t_set_Lall = []\n",
    "for img in annotL_train_df['Image file']:\n",
    "    t_set_Lall.append(img)\n",
    "    if os.path.splitext(img)[0] not in avoid_trimaps:\n",
    "        t_set_Lall.append(os.path.splitext(img)[0]+'_am.jpg')\n",
    "            \n",
    "train_Lall = return_annotations(t_set_Lall)\n",
    "train_Lall_df = train_Lall.create_df() \n",
    "train_Lall_df.to_csv('annotations_aug/annotations_train_Lall.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9bccd7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizeL(train_df): \n",
    "    \n",
    "    len_train = len(train_df)\n",
    "    len_valid = len(imgL_valid)\n",
    "    len_test = len(imgL_test)\n",
    "    total = len_train + len_valid + len_test\n",
    "    \n",
    "    number_edited = sum([os.path.splitext(file)[0].endswith('am') for file in train_df['Image file']])\n",
    "    number_original = len_train - number_edited\n",
    "    \n",
    "    print('There are {}, {}, and {} images in the training, validation, and test sets, respectively.'.format(len_train, len_valid, len_test))\n",
    "    print()\n",
    "    print('The train:valid:test ratio is roughly {:.2f}:{:.2f}:{:.2f}.'.format(len_train*100/total, len_valid*100/total, len_test*100/total))\n",
    "    print()\n",
    "    print('The ratio of original images to the edited ones is about {:.2f}:{:.2f}.'.format(number_original*100/len_train,number_edited*100/len_train))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28ce680e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11132, 919, and 531 images in the training, validation, and test sets, respectively.\n",
      "\n",
      "The train:valid:test ratio is roughly 88.48:7.30:4.22.\n",
      "\n",
      "The ratio of original images to the edited ones is about 53.36:46.64.\n"
     ]
    }
   ],
   "source": [
    "summarizeL(train_L1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb280422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11108, 919, and 531 images in the training, validation, and test sets, respectively.\n",
      "\n",
      "The train:valid:test ratio is roughly 88.45:7.32:4.23.\n",
      "\n",
      "The ratio of original images to the edited ones is about 46.71:53.29.\n"
     ]
    }
   ],
   "source": [
    "summarizeL(train_L2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67740345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11860, 919, and 531 images in the training, validation, and test sets, respectively.\n",
      "\n",
      "The train:valid:test ratio is roughly 89.11:6.90:3.99.\n",
      "\n",
      "The ratio of original images to the edited ones is about 50.08:49.92.\n"
     ]
    }
   ],
   "source": [
    "summarizeL(train_Lall_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84ea9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
